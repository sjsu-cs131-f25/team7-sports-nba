# Team 7 â€” NBA Basketball Analytics

### CS 131 â€” Processing Big Data (Fall 2025)

### Final Project â€” Distributed Data Engineering & Analysis

---

## ğŸ“˜ Project Overview

This project performs large-scale analytics on a historical **NBA dataset (1946â€“present)** ([Kaggle: NBA Basketball Dataset](https://www.kaggle.com/datasets/wyattowalsh/basketball)) using **PySpark**.
We designed and executed a distributed data pipeline to clean, transform, and analyze NBA team and player statistics.
Outputs include processed analytical tables, Spark UI screenshots, and the final written report.

---

## ğŸ‘¥ Team Members

| Name |
 | ----- |
| **Ragavan Arivazhagan** |
| **David Hsiao** |
| **Kyungtae Kim** |
| **Nishan Bhattarai** |
| **Kareem Sheikh** |

---

## ğŸ“Š Dataset Description

### Source

[NBA Historical Dataset](https://www.kaggle.com/datasets/wyattowalsh/basketball)
Contains detailed season-by-season NBA data such as:

* Player-level statistics

* Team performance metrics

* Box scores

* Game logs

* Play-by-play events

### Dataset Card (Sample File)

**Path:** `data/samples/sample_play_by_play.csv`
**Format:** CSV (comma-delimited, UTF-8)
**Rows:** \~1,000
**Columns:** 34
**Header:** Present

The full dataset is assumed to reside under `data/input/` and contains multiple larger CSV files.

---

## ğŸ“ Repository Structure

The project directory structure is as follows:

```
.
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€â”€ instructions.md         # Instructions or metadata about the dataset
â”‚Â Â  â””â”€â”€ samples/
â”‚Â Â   Â  Â  â””â”€â”€ sample_play_by_play.csv # Small sample data file for local testing
â”‚
â”œâ”€â”€ final_pipeline.py           # Main PySpark script for distributed data processing
â”œâ”€â”€ Final_project_new_analysis.ipynb # Primary analysis and visualization notebook
â”œâ”€â”€ logs/
â”‚Â Â  â””â”€â”€ errors.log              # Log file capturing errors or job outputs
â”‚
â”œâ”€â”€ notebook/                   # Directory used for intermediate notebooks or scratchpad
â”œâ”€â”€ out/                        # Contains all final, processed, and aggregated results
â”‚Â Â  â”œâ”€â”€ Bar chart fouls_per_game_team.png # Visualization output
â”‚Â Â  â”œâ”€â”€ Barchart_avgfouls_per_player_per_game.png
â”‚Â Â  â”œâ”€â”€ clean/
â”‚Â Â  â”‚Â Â  â””â”€â”€ clean_play_by_play.csv # The main cleaned dataset
â”‚Â Â  â”œâ”€â”€ cluster_histogram.png
â”‚Â Â  â”œâ”€â”€ cluster_outcomes.tsv    # (and many other analysis outputs/charts)
â”‚Â Â  â””â”€â”€ top30_overall.txt
â”‚
â”œâ”€â”€ Project_Assignment_5_.ipynb # Intermediate assignment/analysis notebook (Version 1)
â”œâ”€â”€ Project_Assignment_5.ipynb  # Intermediate assignment/analysis notebook (Version 2)
â”œâ”€â”€ project2_session.txt        # Session log/output for a specific run
â”œâ”€â”€ README.md
â”œâ”€â”€ run_pa4.sh                  # Shell script to run Assignment 4 pipeline
â”œâ”€â”€ run_project2.sh             # Shell script to run Project 2 main pipeline
â”œâ”€â”€ scripts/                    # Contains reusable shell scripts for job execution
â”‚Â Â  â”œâ”€â”€ eng1_edges.sh
â”‚Â Â  â”œâ”€â”€ run_pa3.sh
â”‚Â Â  â”œâ”€â”€ sample_data.sh
â”‚Â Â  â””â”€â”€ skinny_table.sh
â””â”€â”€ Sprint_6_Step3_Step4_Final_ipynb.ipynb # Final processing steps notebook
```

---

## ğŸš€ How to Run the Distributed Job

The primary data transformation logic is in `final_pipeline.py` and is typically executed via the shell scripts in the root directory.

### 1. Prerequisites

* Python 3.x

* PySpark

* Local Spark OR access to a distributed Spark cluster

### 2. Run Locally (Standalone Spark)

From the repository root, execute the main shell script, or run `final_pipeline.py` directly:

```bash
# Option A: Run via the shell script (recommended, as it handles parameters)
./run_project2.sh 

# Option B: Direct spark-submit using the main PySpark file
spark-submit final_pipeline.py \
--input data/input/ \
--output data/out/
```

This process loads raw data (assumed to be in `data/input/`) and writes results into `data/out/`.

### 3. Run on Any Spark Cluster

```bash
spark-submit \
--master <your-cluster-master> \
--deploy-mode client \
final_pipeline.py \
--input <input-path> \
--output <output-path>
```

Examples for `<input-path>` and `<output-path>`:

* `gs://your-bucket/input/` and `gs://your-bucket/out/`

* `hdfs:///user/team7/input/` and `hdfs:///user/team7/out/`

### 4. Optional: Dataproc Serverless Execution

```bash
gcloud dataproc batches submit spark \
--region=us-central1 \
--batch=team7-nba-run \
--execute final_pipeline.py \
-- \
gs://your-bucket/input/ \
gs://your-bucket/out/
```

## ğŸ“¥ Input Data Location

Local input path (assumed):
`data/input/`

Cluster / Cloud input path:
`gs://your-bucket/input/`

Contents include:

* Player statistics

* Team statistics

* Game logs

* Play-by-play datasets

## ğŸ“¤ Output Data Location

Local output path:
`data/out/`

Cluster / Cloud output path:
`gs://your-bucket/out/`

Outputs include:

* Cleaned datasets (in `data/out/clean/`)

* Aggregated analysis tables (e.g., `top30_overall.txt`, `entity_counts.tsv`)

* Visualization charts (e.g., all `.png` files)

## ğŸ“ˆ Spark UI Evidence

A distributed Spark run includes:

* **Jobs tab** â€” stage and task breakdown

* **SQL tab** â€” physical execution plan

* **Executors tab** â€” resource usage metrics

Screenshots of the Spark UI are typically included in the final report (assumed to exist outside of this file structure).

## ğŸ¤– AI Tooling Disclosure

AI tools (e.g., ChatGPT) were used **selectively and minimally**, specifically for:

* Clarifying Spark error messages

* Understanding PySpark configuration flags

* Improving documentation clarity

All code and analytic logic were manually written, tested, and validated by the team.

## ğŸ“š License

This project is for academic use as part of **CS 131 â€” Processing Big Data**.

