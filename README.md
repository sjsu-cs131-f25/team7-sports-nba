# Team 7 â€” NBA Basketball Analytics

### CS 131 â€” Processing Big Data (Fall 2025)

### Final Project â€” Distributed Data Engineering & Analysis

---

## ğŸ“˜ Project Overview

This project performs large-scale analytics on a historical **NBA dataset (1946â€“present)** using **PySpark**.
We designed and executed a distributed data pipeline to clean, transform, and analyze NBA team and player statistics.
Outputs include processed analytical tables, Spark UI screenshots, and the final written report.

---

## ğŸ‘¥ Team Members & Roles

| Name | 
 | ----- | 
| **Ragavan Arivazhagan** |
| **David Hsiao** |
| **Kyungtae Kim** |
| **Nishan Bhattarai** |
| **Kareem Sheikh** | 

---

## ğŸ“Š Dataset Description

### Source

Kaggle â€“ *NBA Historical Dataset*
Contains detailed season-by-season NBA data such as:

* Player-level statistics

* Team performance metrics

* Box scores

* Game logs

* Play-by-play events

### Dataset Card (Sample File)

**Path:** `team7-sports-nba/data/samples/sample_play_by_play.csv`
**Format:** CSV (comma-delimited, UTF-8)
**Rows:** \~1,000
**Columns:** 34
**Header:** Present

The full dataset lives under `data/input/` and contains multiple larger CSV files.

---

## ğŸ“ Repository Structure

```
team7-sports-nba/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ main.py               # Main PySpark processing script
â”‚   â””â”€â”€ utils/                # Helper modules (if used)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                # Raw NBA datasets (CSV/Parquet)
â”‚   â”œâ”€â”€ output/               # Results generated by Spark
â”‚   â””â”€â”€ samples/              # Small sample files for testing
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ final_report.pdf      # Final project report
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run the Distributed Job

Our pipeline logic lives in:
`code/main.py`

It loads input data, applies transformations, and writes results to the output folder.

### 1. Prerequisites

* Python 3.x

* PySpark

* Local Spark OR access to a distributed Spark cluster

### 2. Run Locally (Standalone Spark)

From the repository root:

```bash
spark-submit code/main.py \
--input data/input/ \
--output data/output/
```

This command:

* Loads raw data from `data/input/`

* Runs all data transformations and aggregations

* Writes the outputs into `data/output/`

### 3. Run on Any Spark Cluster

```bash
spark-submit \
--master <your-cluster-master> \
--deploy-mode client \
code/main.py \
--input <input-path> \
--output <output-path>
```

Examples:

* `gs://your-bucket/input/`

* `hdfs:///user/team7/input/`

* `s3://your-bucket/input/`

### 4. Optional: Dataproc Serverless Execution

```bash
gcloud dataproc batches submit spark \
--region=us-central1 \
--batch=team7-nba-run \
--execute code/main.py \
-- \
gs://your-bucket/input/ \
gs://your-bucket/output/
```

## ğŸ“¥ Input Data Location

Local input path:
`data/input/`

Cluster / Cloud input path:
`gs://your-bucket/input/`

Contents include:

* Player statistics

* Team statistics

* Game logs

* Play-by-play datasets

## ğŸ“¤ Output Data Location

Local output path:
`data/output/`

Cluster / Cloud output path:
`gs://your-bucket/output/`

Outputs include:

* Cleaned datasets

* Aggregated tables

* Player/team performance summaries

## ğŸ“ˆ Spark UI Evidence

A distributed Spark run includes:

* **Jobs tab** â€” stage and task breakdown

* **SQL tab** â€” physical execution plan

* **Executors tab** â€” resource usage metrics

Screenshots of the Spark UI are included in the final report.

## ğŸ¤– AI Tooling Disclosure

AI tools (e.g., ChatGPT) were used **selectively and minimally**, specifically for:

* Clarifying Spark error messages

* Understanding PySpark configuration flags

* Improving documentation clarity

All code and analytic logic were manually written, tested, and validated by the team.

## ğŸ“š License

This project is for academic use as part of **CS 131 â€” Processing Big Data**.
